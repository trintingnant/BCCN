{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual Decision Making\n",
    "Sudeshna Bora & Leonard Hollander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise 13.1: the network implementation}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Exercise 13.1.1. Question: Understanding Brian2 Monitors}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable names are $A, B, Z$  and $\\textbf{inhib}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the different monitors are simply the denoted by adding the name of the population to the prefixes: $\\textbf{\"spike_monitor_ \"}$, $\\textbf{\"rate_monitor_ \"}$ and $\\textbf{\"voltage_monitor_ \"}$, where the name of the population is specified in the end.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state monitor records the temporal evolution of the neuron voltage $\\textbf{v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brian2 as b2\n",
    "from neurodynex.tools import plot_tools\n",
    "from neurodynex.competing_populations import decision_making as DM\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{13.1.2. Question: Accessing a dictionary to plot the population rates}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define experiments parameters:\n",
    "trial_time = 800 * b2.ms\n",
    "#running simulation:\n",
    "results = DM.sim_decision_making_network(t_stimulus_start= 50. * b2.ms,\\\n",
    "                                                      coherence_level=-0.6, max_sim_time=1000. * b2.ms)                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_wds = np.zeros(11)\n",
    "avg_wds[:-1] = np.arange(.1,5.1,.5) #different window sizes\n",
    "avg_wds[-1] = 50\n",
    "#plotting results:\n",
    "def plot_sim_results(avg_wdw: float, results:dict=results) -> None:\n",
    "    \n",
    "    print('Simulation with window width: {}ms'.format(avg_wdw))\n",
    "    plot_tools.plot_network_activity(results[\"rate_monitor_A\"], results[\"spike_monitor_A\"],\n",
    "                                 results[\"voltage_monitor_A\"], t_min=0. * b2.ms, avg_window_width=avg_wdw *b2.ms,\n",
    "                                 sup_title=\"Left with width {}ms\".format(avg_wdw))\n",
    "    \n",
    "    plot_tools.plot_network_activity(results[\"rate_monitor_B\"], results[\"spike_monitor_B\"],\n",
    "                                 results[\"voltage_monitor_B\"], t_min=0. * b2.ms, avg_window_width=avg_wdw *b2.ms,\n",
    "                                 sup_title=\"Right with width {}ms\".format(avg_wdw))\n",
    "    \n",
    "    plot_tools.plot_network_activity(results[\"rate_monitor_inhib\"], results[\"spike_monitor_inhib\"],\n",
    "                                 results[\"voltage_monitor_inhib\"], t_min=0. * b2.ms, avg_window_width=avg_wdw *b2.ms,\n",
    "                                 sup_title=\"Inhib with width {}ms\".format(avg_wdw))\n",
    "    \n",
    "    plot_tools.plot_network_activity(results[\"rate_monitor_Z\"], results[\"spike_monitor_Z\"],\n",
    "                                 results[\"voltage_monitor_Z\"], t_min=0. * b2.ms, avg_window_width=avg_wdw *b2.ms,\n",
    "                                 sup_title=\"Z with width {}ms\".format(avg_wdw))\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "for idx, avg_wdw in np.ndenumerate(avg_wds):\n",
    "    \n",
    "    plot_sim_results(avg_wdw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Answer:}$ We can see that as we increase the average window size, the firing rates become more continuous looking curves. It's fairly intuitive that at small window sizes the firing rates approximate $\\delta$ pulses, since, on average, only single spikes fall into a single window. At the other extreme, taking a window size equal to the trial lenght would be equivalent to recording the mean spike count rate, i.e. the number of spikes occuring over the time of the trial, divided by that time.\n",
    "\n",
    "\n",
    "At $5ms$ the overall curves already look pretty reasonable in our eyes. Qualitatively, they match our expectations, which is that after a while the firing rates in the specific neural populations $A$ and $B$ tend to either a continuous firing activity or silence. At the prescribed coherence level the right population should show heightened activity while the left population should 'quiet down'. \n",
    "\n",
    "Taking the average across much larger intervals smoothens out the curves considerable to reveal long-term trends. At $50ms$ we can clearly see the ramp-up and attenuation play out on a relatively large timescale. Both kinds of information are valuable, however. In partciular, the choice of an adequate decision criterion in later parts of the exercise will depend on the choice of window size. We think it best to use intermediate windowsizes, because they balance locally available information (e.g. information about short spike \"bursts\") against long term trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{13.2. Exercise: Stimulating the decision makin circuit}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{13.2.1. Question: Coherence Level}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "\\mu_{left} - \\mu_{right} = \\mu_0 \\cdot c\n",
    "\\end{equation}$\n",
    "\n",
    "Since the difference of two Gaussian is a Gaussian with 'subtracted' means and 'added' variances we have\n",
    "\n",
    "$\\begin{equation}\n",
    "\\mu_{left} - \\mu_{right} \\sim \\mathcal{N}(\\mu_{left} - \\mu_{right}, 2\\sigma2)\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Default values:}$ The default values are 160 and 20 Hz respectively for $\\mu_0$ and $\\sigma$. With these values we can calculate $\\mu_{left}$ and $\\mu_{right}$ analytically from the equations above: \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu_{left} &= 64Hz \\\\\n",
    "\\mu_{right} &= 96Hz\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "At $c=-0.2$, we have $\\mu_{left} - \\mu_{right}$=-32Hz (indicating we will have a decision favouring the right population). The variance $\\sigma=40Hz$. We can thus see that the variance is quite large compared with the mean, indicating that noise can perturb the system towards the wrong decision. This is intuitive, given the relatively small coherence $c$ of dot motion. If we had perfect coherence (1/-1) we would have a much larger mean (160Hz). In this case, the system always makes the correct decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{13.2.2. Question: Input stimuli with different coherence levels}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulations with c=0.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default window width (from above):\n",
    "def_window_width = avg_wds[-1] #5ms window\n",
    "\n",
    "for i in np.arange(5): #run five simulations\n",
    "\n",
    "    #coherence level -.1:\n",
    "    results01=DM.sim_decision_making_network(t_stimulus_start= 50. * b2.ms,\\\n",
    "                                                      coherence_level=.6, max_sim_time=1000. * b2.ms)   \n",
    "    plot_sim_results(def_window_width, results01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulations with c=-0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.arange(5): #run five simulations\n",
    "\n",
    "    #coherence level -.1:\n",
    "    results06=DM.sim_decision_making_network(t_stimulus_start= 50. * b2.ms,\\\n",
    "                                                      coherence_level=-0.1, max_sim_time=1000. * b2.ms)\n",
    "\n",
    "    plot_sim_results(def_window_width, results06)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already mentioned that at low coherence levels, the system should be expected, on some occassions, (1) either to not have enough time to settle into one of the attractors or (2) possibly even make the wrong decision. In some trials, most notably the first and the last), while we do see the correct ramp-up/attenuation of firing rates in the two populations respectively, it seems as though the activity in the right subpopulation might not (depending on the stringency of our decision criterion) be large enough to say that the system made a decision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_window_width = 123*b2.ms\n",
    "sr = results[\"rate_monitor_A\"].smooth_rate(window=\"flat\", width=avg_window_width)/b2.Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dSpace (r_mon1, \n",
    "                 r_mon2,\n",
    "                 xlabel: str= 'Firing rate left',\n",
    "                 ylabel: str= 'Firing rate right',\n",
    "                 avg_window_width=avg_window_width) -> None:\n",
    "    \n",
    "     #Get time points to colour code temporal progression of firing rate\n",
    "\n",
    "     m1 = r_mon1.smooth_rate(window=\"flat\", width=avg_window_width)\n",
    "     m2 = r_mon2.smooth_rate(window=\"flat\", width=avg_window_width)\n",
    "\n",
    "    \n",
    "     #Get time points to colour code temporal progression of firing rate\n",
    "     time_ax = np.linspace(0,trial_time / b2.ms, len(m1))\n",
    "     #Plot the results:\n",
    "     plt.figure()\n",
    "     plt.scatter(m1,m2,c=time_ax, cmap='coolwarm')\n",
    "     plt.title('Decision Space for width={}s'.format(avg_window_width))\n",
    "     plt.xlabel(xlabel)\n",
    "     plt.ylabel(ylabel)\n",
    "     plt.colorbar(label='Time in $ms$')\n",
    "     \n",
    "     #plt.show()\n",
    "\n",
    "     return None\n",
    "\n",
    "#Plot decision space for various window_widths\n",
    "\n",
    "avg_wds = (1,5,20,50,100)\n",
    "for wdw in avg_wds:\n",
    "    \n",
    "    plot_dSpace(results[\"rate_monitor_A\"],\\\n",
    "        results[\"rate_monitor_B\"], avg_window_width=wdw * b2.ms)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think this nicely illustrates something we already mentioned earlier, namely that an increased window size allows the firing rates to look more like a smooth function rather than a series of $\\delta$ spikes. \n",
    "\n",
    "In this case we can clearly see that the system settles in the right attractor, i.e. the upper left corner of the decision space. Initially, the firing rate on the left side increases, then the firing rate of the right population begins to increase, inhibiting the left population driving the system to the upper left corner. The rate at which this corner is approached seems to increase with time. Presumably this is because of exisiting positive feedback loops. The right population inhibits the the left population blocking its own inhibition by said left population, thereby increasing its own activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{13.3.2. Question: Implementing a decision criterion}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we may try with a constant threshhold that we derive empirically from our experiemnts above. In those it always seemed that the 'favoured' subpopulation settled in a frequency band around 50Hz (excited) and the other subpopulation's activity was attenuated successively to eventually become 0. If we apply this decision criterion, then our threshhold should be something like 50Hz. But this will only work for window sizes that are in the ballpark of what we used earlier. If we choose the windowsize too large, our result have a lot of global, but very little local information.\n",
    "\n",
    "One could also try to determine the threshold analytically by taking performing a one-sided t-Test on intervals of length $\\Delta t$ (window size; p-value can freely adjustable) and seeing whether that test ever allowed one to discard the null-hypthosis (firing rate  $\\sim \\mu_0$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "def get_decision_time (r_mon1,\n",
    "                      r_mon2,\n",
    "                      avg_window_width=5*b2.ms,\n",
    "                      rthresh=50*b2.Hz):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the decision time:\n",
    "    @params:\n",
    "    r_mon1: rate monitor\n",
    "    r_mon2: rate monitor\n",
    "    avg_window_width: window_width across \n",
    "    whicht spikes are averaged\n",
    "    rthresh: firing rate threshold (decision?)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m1 = r_mon1.smooth_rate(window=\"flat\", width=avg_window_width)\n",
    "    m2 = r_mon2.smooth_rate(window=\"flat\", width=avg_window_width)\n",
    "    \n",
    "    #Filter by threshold:\n",
    "    m1, m2 = np.where(m1 > rthresh)[0], np.where(m2 > rthresh)[0]\n",
    "    \n",
    "    #make sure decision is made\n",
    "    assert(len(m1)==0 or len(m2)==0), \"Decision Failure: check rate threshhold\"\n",
    "    \n",
    "    return (0 if len(m1)==0 else np.min(m1) * b2.defaultclock.dt,\\\n",
    "            0 if len(m2)==0 else np.min(m2) * b2.defaultclock.dt)\n",
    "\n",
    "get_decision_time(results[\"rate_monitor_B\"],\\\n",
    "                  results[\"rate_monitor_A\"])\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{13.4.1. Question: Running multiple simulations}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def perc_corr(coherence_levels,\n",
    "             time_to_A,\n",
    "             time_to_B,\n",
    "             count_A,\n",
    "             count_B,\n",
    "             count_No,\n",
    "             no_decisions: bool=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the percentage of correct and incorrect decisions:\n",
    "    @params:\n",
    "    coherence_levels: a list of coherence levels\n",
    "    time_to_A: time stamps of decisions for A\n",
    "    time_to_B: time stamps of decisions for B\n",
    "    count_A: no of times decision A was made\n",
    "    count_B: no of times decision B was made\n",
    "    no_decisions: determines whether refraining should\n",
    "    always count as having made an incorrect decision.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nLevels, nReps = time_to_A.shape\n",
    "    result = np.zeros((nLevels, 2))\n",
    "    \n",
    "    \n",
    "    for idx, coLev in np.ndenumerate(coherence_levels):\n",
    "        \n",
    "        i = 0 if coLev > 0 else 1 #A is correct when coLev > 1, otherwise B is correct\n",
    "        \n",
    "        #count correct and incorrect decisions\n",
    "        times_correct, times_wrong = (count_A[idx], count_B[idx])[::np.power(-1,i)]\n",
    "        \n",
    "        #Calculate percentages\n",
    "        if no_decisions:\n",
    "            result[idx,:] = np.array([times_correct, nReps - times_correct]) / nReps\n",
    "        \n",
    "        else:\n",
    "            count_reps = (nReps - count_No)[idx] #count valid trials\n",
    "            result[idx,:] = np.array([times_correct, times_wrong]) / count_reps\n",
    "        \n",
    "    return result\n",
    "\n",
    "#Percentages when withholding a decision isn't counted:\n",
    "percentages = perc_corr(coherence_levels, \n",
    "             time_to_A, \n",
    "             time_to_B, \n",
    "             count_A, \n",
    "             count_B,\n",
    "             count_No,\n",
    "             no_decisions=False)\n",
    "\n",
    "percentages = pd.DataFrame(percentages,\\\n",
    "                          columns=['incorrect', 'correct'][::-1],\\\n",
    "                          index=coherence_levels)\n",
    "\n",
    "print('Percentages with non-decisions not counted: ')\n",
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentages when withholding a decision counts as wrong:      \n",
    "percentages_noDecWrong = perc_corr(coherence_levels, \n",
    "             time_to_A, \n",
    "             time_to_B, \n",
    "             count_A, \n",
    "             count_B,\n",
    "             count_No,\n",
    "             no_decisions=True)\n",
    "\n",
    "percentages_noDecWrong = pd.DataFrame(percentages_noDecWrong,\\\n",
    "                          columns=['incorrect', 'correct'][::-1],\\\n",
    "                          index=coherence_levels)\n",
    "\n",
    "print('Percentages with non-decisions counted as wrong: ')\n",
    "percentages_noDecWrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see our earlier suspicion confirmed. For small coherence levels, the system actually mostly withholds making a decision. The choice of simulation time, rate threshhold and window size contribute to this result. With a longer integration time, the system may be able to make more (ultimately correct decisions). Choice becomes more unreliable with a smaller rate threshhold. Our choice of threshhold seems to be a rather conserative one, since the system mostly foregoes making a choice. It is surprising therefore that it nonetheless makes quite a lot of wrong decisions. The system here seems to be at chance level. However, we surmise that this is a statistical artefact, due to the limited number of simulations we are able to perform.\n",
    "\n",
    "At larger levels of coherence the system has a perfect performance when we black out trials in which no decision is made. Overall, the system behaves according to our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define experiment parameters:\n",
    "\n",
    "coherence_levels = [.15,-.8]\n",
    "nreps = 15\n",
    "avg_window_width = 5 * b2.ms\n",
    "max_sim_time = 1000 * b2.ms\n",
    "rThresh = 50 * b2.Hz\n",
    "\n",
    "#run simulation:\n",
    "time_to_A,time_to_B, count_A, count_B, count_No = \\\n",
    "DM.run_multiple_simulations(get_decision_time,\\\n",
    "                            coherence_levels,\\\n",
    "                            nreps,\\\n",
    "                            max_sim_time=max_sim_time,\\\n",
    "                            rate_threshold = rThresh,\\\n",
    "                            avg_window_width=avg_window_width)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_A = time_to_A.sum(axis=1) / count_A\n",
    "np.where(np.isnan(time_A),0,  time_A)\n",
    "time_to_B.sum(axis=1) / count_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_simulation_stats(coherence_levels, \n",
    "                          time_to_A, \n",
    "                          time_to_B, \n",
    "                          count_A, \n",
    "                          count_B) -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Plots a subplot consisting of three graphs:\n",
    "        1. percentage correct vs. coherence level (no decisions counted)\n",
    "        2. percentage correct vs. coherence level (no decisions not counted)\n",
    "        3. Integration time vs. coherence level\n",
    "        \n",
    "        @params: ...\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #get integration times:\n",
    "    \n",
    "    time_A = time_to_A.sum(axis=1) / count_A\n",
    "    time_A = np.where(np.isnan(time_A), 1,  time_A)\n",
    "    time_B = time_to_B.sum(axis=1) / count_B\n",
    "    \n",
    "    \n",
    "    fig, axs = plt.subplots(1,3, figsize=(10,5))\n",
    "    plt.tight_layout(h_pad=20, w_pad=2)\n",
    "\n",
    "    \n",
    "    axs[0].set_title('not counting no-decision trials')\n",
    "    axs[0].plot(coherence_levels, percentages.values[:,0]*100, c='tomato')\n",
    "    axs[0].set_xlabel('coherence level $c$')\n",
    "    axs[0].set_ylabel('correct decisions in $\\%$')\n",
    "    axs[1].set_title('counting no-decision trials')\n",
    "    axs[1].plot(coherence_levels, percentages_noDecWrong.values[:,0]*100, c='tomato')\n",
    "    axs[1].set_xlabel('coherence level $c$')\n",
    "    axs[1].set_ylabel('correct decisions in $\\%$')\n",
    "    axs[2].set_title('coherence levels vs. integration time ')\n",
    "    axs[2].plot(coherence_levels, time_A, c='tomato', label='integration time A')\n",
    "    axs[2].plot(coherence_levels, time_B, c='steelblue', label='integration time B')\n",
    "    axs[2].set_xlabel('coherence level $c$')\n",
    "    axs[2].set_ylabel('integration time')\n",
    "    axs[2].legend()\n",
    "    \n",
    "    \n",
    "    return None\n",
    "\n",
    "plot_simulation_stats(coherence_levels,\n",
    "                     time_to_A,\n",
    "                     time_to_B,\n",
    "                     count_A,\n",
    "                     count_B)\n",
    "  \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These figures summarize the simulation results. Firstly, we can see that the percentage of correct decisions (on any way of counting) decreases as we decrease the coherence. Secondly, we can see that integration time increases. Moreover, looking at the integration time curves we can see that integration time is decreased for incorrect as opposed to correct decisions (since at $c=0.8$ decision A was never made, we capped integration time at 1s for the sake of the figure, but actually it should be understood that the integration time there is $\\infty$). This replicates one key finding from the psychophysical experiments on decision making that were already mentioned in the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
