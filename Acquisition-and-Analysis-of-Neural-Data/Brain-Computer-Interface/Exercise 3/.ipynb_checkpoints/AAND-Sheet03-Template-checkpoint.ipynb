{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General rules:\n",
    " * For all figures that you generate, remember to add meaningful labels to the axes, and make a legend, if applicable.\n",
    " * Do not hard code constants, like number of samples, number of channels, etc in your program. These values should always be determined from the given data. This way, you can easily use the code to analyse other data sets.\n",
    " * Do not use high-level functions from toolboxes like scikit-learn.\n",
    " * Replace *Template* by your *FirstnameLastname* in the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAND - BCI Exercise Sheet #03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import bci_minitoolbox as bci "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Nearest Centroid Classifier (NCC)  (1 point)\n",
    "Implement the calculation of the nearest centroid classifier (NCC) as a Python function `train_NCC`.  The function should take two arguments, the first being the data matrix $\\bf{X}$ where each column is a data point ($\\bf{x_k}$), and the second being class labels of the data points. Two output arguments should return the weight vector **`w`** and bias `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NCC(X, y):\n",
    "    '''\n",
    "    Synopsis:\n",
    "        w, b= train_NCC(X, y)\n",
    "    Arguments:\n",
    "        X: data matrix (features X samples)\n",
    "        y: labels with values 0 and 1 (1 x samples)\n",
    "    Output:\n",
    "        w: NCC weight vector\n",
    "        b: bias term\n",
    "    '''\n",
    "    \n",
    "    labels = [0,1]\n",
    "    mean1, mean2 = [X.T[y==i].mean(axis=0) for i in labels]\n",
    "    \n",
    "    #Calculate weights and bias:\n",
    "    w = mean2 - mean1\n",
    "    b = w.T @ ((mean1 + mean2) / 2)\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Linear Discriminant Analysis (LDA)  (3 points)\n",
    "Implement the calculation of the LDA classifier as a Python function `train_LDA`.  The function should take two arguments, the first being the data matrix $\\bf{X}$ where each column is a data point ($\\bf{x_k}$), and the second being class labels of the data points. Two output arguments should return the weight vector **`w`** and bias `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_LDA(X, y):\n",
    "    '''\n",
    "    Synopsis:\n",
    "        w, b= train_LDA(X, y)\n",
    "    Arguments:\n",
    "        X: data matrix (features X samples)\n",
    "        y: labels with values 0 and 1 (1 x samples)\n",
    "    Output:\n",
    "        w: LDA weight vector\n",
    "        b: bias term\n",
    "    '''\n",
    "    \n",
    "    covMat = np.eye(X.shape[0]) @ X.std(axis=1)\n",
    "    mean1, mean2  = [X.T[y==i].mean(axis=0) for i in labels]\n",
    "    \n",
    "    #Calculate weights and bias\n",
    "    w = np.linalg.inv(covMat) @ (mean2 - mean1)\n",
    "    b = w.T @ ((mean1 + mean2) / 2)\n",
    "    \n",
    "    return w, b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises 3: Cross-validation with weighted loss (1 point)\n",
    "Complete the implementation of `crossvalidation` by writing a loss function `loss_weighted_error` which calculates the weighted loss as explained in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crossvalidation(classifier_fcn, X, y, nFolds=10, verbose=False):\n",
    "    '''\n",
    "    Synopsis:\n",
    "        loss_te, loss_tr= crossvalidation(classifier_fcn, X, y, nFolds=10, verbose=False)\n",
    "    Arguments:\n",
    "        classifier_fcn: handle to function that trains classifier as output w, b\n",
    "        X:              data matrix (features X samples)\n",
    "        y:              labels with values 0 and 1 (1 x samples)\n",
    "        nFolds:         number of folds\n",
    "        verbose:        print validation results or not\n",
    "    Output:\n",
    "        loss_te: value of loss function averaged across test data\n",
    "        loss_tr: value of loss function averaged across training data\n",
    "    '''\n",
    "    nDim, nSamples = X.shape\n",
    "    inter = np.round(np.linspace(0, nSamples, num=nFolds + 1)).astype(int)\n",
    "    perm = np.random.permutation(nSamples)\n",
    "    errTr = np.zeros([nFolds, 1])\n",
    "    errTe = np.zeros([nFolds, 1])\n",
    "\n",
    "    for ff in range(nFolds):\n",
    "        idxTe = perm[inter[ff]:inter[ff + 1] + 1]\n",
    "        idxTr = np.setdiff1d(range(nSamples), idxTe)\n",
    "        w, b = classifier_fcn(X[:, idxTr], y[idxTr])\n",
    "        out = w.T.dot(X) - b\n",
    "        errTe[ff] = loss_weighted_error(out[idxTe], y[idxTe])\n",
    "        errTr[ff] = loss_weighted_error(out[idxTr], y[idxTr])\n",
    "\n",
    "    if verbose:\n",
    "        print('{:5.1f} +/-{:4.1f}  (training:{:5.1f} +/-{:4.1f})  [using {}]'.format(errTe.mean(), errTe.std(),\n",
    "                                                                                     errTr.mean(), errTr.std(), \n",
    "                                                                                     classifier_fcn.__name__))\n",
    "    return np.mean(errTe), np.mean(errTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_weighted_error(out, y):\n",
    "    '''\n",
    "    Synopsis:\n",
    "        loss= loss_weighted_error( out, y )\n",
    "    Arguments:\n",
    "        out:  output of the classifier\n",
    "        y:    true class labels\n",
    "    Output:\n",
    "        loss: weighted error\n",
    "    '''\n",
    "    \n",
    "    labels = [0,1]\n",
    "    err0, err1 = [(out[y==lab] - y[y==lab]).mean()**2 if len(y[y==lab]) != 0 else 0 \\\n",
    "                  for lab in labels]\n",
    "    \n",
    "    return .5 * (err0 + err1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = 'erp_hexVPsag.npz'\n",
    "cnt, fs, clab, mnt, mrk_pos, mrk_class, mrk_className = bci.load_data(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Classification of Temporal Features  (2 points)\n",
    "Extract as temporal features from single channels the epochs of the time interval 0 to 1000 ms. Determine the error of classification with LDA and with NCC on those features using 10-fold cross-validation for each single channel. Display the resulting (test) error rates for all channel as scalp topographies (one for LDA and one for NCC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Classification of Spatial Features  (3 points)\n",
    "Perform classification (*target* vs. *nontarget*) on spatial features (average across time within a 50 ms interval) in a time window that is shifted from 0 to 1000 ms in steps of 10 ms, again with both, LDA and NCC. Visualize the time courses of the classification error. Again, use 10-fold cross-validation. Here, use a baseline correction w.r.t. the prestimulus interval -100 to 0 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = np.ones(5)*2\n",
    "a = np.hstack((a,3))\n",
    "np.unique(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
