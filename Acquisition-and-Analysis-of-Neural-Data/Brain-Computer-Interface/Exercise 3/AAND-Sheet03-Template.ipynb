{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General rules:\n",
    " * For all figures that you generate, remember to add meaningful labels to the axes, and make a legend, if applicable.\n",
    " * Do not hard code constants, like number of samples, number of channels, etc in your program. These values should always be determined from the given data. This way, you can easily use the code to analyse other data sets.\n",
    " * Do not use high-level functions from toolboxes like scikit-learn.\n",
    " * Replace *Template* by your *FirstnameLastname* in the filename."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAND - BCI Exercise Sheet #03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import bci_minitoolbox as bci "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Nearest Centroid Classifier (NCC)  (1 point)\n",
    "Implement the calculation of the nearest centroid classifier (NCC) as a Python function `train_NCC`.  The function should take two arguments, the first being the data matrix $\\bf{X}$ where each column is a data point ($\\bf{x_k}$), and the second being class labels of the data points. Two output arguments should return the weight vector **`w`** and bias `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NCC(X, y):\n",
    "    '''\n",
    "    Synopsis:\n",
    "        w, b= train_NCC(X, y)\n",
    "    Arguments:\n",
    "        X: data matrix (features X samples)\n",
    "        y: labels with values 0 and 1 (1 x samples)\n",
    "    Output:\n",
    "        w: NCC weight vector\n",
    "        b: bias term\n",
    "    '''\n",
    "    \n",
    "    labels = [0,1]\n",
    "    mean1, mean2 = [X[y==i].mean(axis=1) for i in labels]\n",
    "    \n",
    "    #Calculate weights and bias:\n",
    "    w = mean2 - mean1\n",
    "    print(\"w: \", w)\n",
    "    b = w.T @ ((mean1 + mean2) / 2)\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Linear Discriminant Analysis (LDA)  (3 points)\n",
    "Implement the calculation of the LDA classifier as a Python function `train_LDA`.  The function should take two arguments, the first being the data matrix $\\bf{X}$ where each column is a data point ($\\bf{x_k}$), and the second being class labels of the data points. Two output arguments should return the weight vector **`w`** and bias `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LDA(X, y):\n",
    "    '''\n",
    "    Synopsis:\n",
    "        w, b= train_LDA(X, y)\n",
    "    Arguments:\n",
    "        X: data matrix (features X samples)\n",
    "        y: labels with values 0 and 1 (1 x samples)\n",
    "    Output:\n",
    "        w: LDA weight vector\n",
    "        b: bias term\n",
    "    '''\n",
    "    labels = [0,1]\n",
    "    covMat = np.eye(X.shape[0]) @ X.std(axis=1)\n",
    "    mean1, mean2  = [X[y==i].mean(axis=1) for i in labels]\n",
    "    \n",
    "    #Calculate weights and bias\n",
    "    print(\"mean1:\", mean1, \"mean2:\", mean2)\n",
    "    w = np.linalg.inv(covMat) @ (mean2 - mean1)\n",
    "    \n",
    "    b = w.T @ ((mean1 + mean2) / 2)\n",
    "    \n",
    "    return w, b\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises 3: Cross-validation with weighted loss (1 point)\n",
    "Complete the implementation of `crossvalidation` by writing a loss function `loss_weighted_error` which calculates the weighted loss as explained in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation(classifier_fcn, X, y, nFolds=10, verbose=False):\n",
    "    '''\n",
    "    Synopsis:\n",
    "        loss_te, loss_tr= crossvalidation(classifier_fcn, X, y, nFolds=10, verbose=False)\n",
    "    Arguments:\n",
    "        classifier_fcn: handle to function that trains classifier as output w, b\n",
    "        X:              data matrix (features X samples)\n",
    "        y:              labels with values 0 and 1 (1 x samples)\n",
    "        nFolds:         number of folds\n",
    "        verbose:        print validation results or not\n",
    "    Output:\n",
    "        loss_te: value of loss function averaged across test data\n",
    "        loss_tr: value of loss function averaged across training data\n",
    "    '''\n",
    "    nDim, nSamples = X.shape\n",
    "    inter = np.round(np.linspace(0, nSamples, num=nFolds + 1)).astype(int)\n",
    "    perm = np.random.permutation(nSamples)\n",
    "    errTr = np.zeros([nFolds, 1])\n",
    "    errTe = np.zeros([nFolds, 1])\n",
    "\n",
    "    for ff in range(nFolds):\n",
    "        idxTe = perm[inter[ff]:inter[ff + 1] + 1]\n",
    "        idxTr = np.setdiff1d(range(nSamples), idxTe)\n",
    "        w, b = classifier_fcn(X[:, idxTr], y[idxTr])\n",
    "        out = w.T.dot(X) - b\n",
    "        errTe[ff] = loss_weighted_error(out[idxTe], y[idxTe])\n",
    "        errTr[ff] = loss_weighted_error(out[idxTr], y[idxTr])\n",
    "\n",
    "    if verbose:\n",
    "        print('{:5.1f} +/-{:4.1f}  (training:{:5.1f} +/-{:4.1f})  [using {}]'.format(errTe.mean(), errTe.std(),\n",
    "                                                                                     errTr.mean(), errTr.std(), \n",
    "                                                                                     classifier_fcn.__name__))\n",
    "    return np.mean(errTe), np.mean(errTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_weighted_error(out, y):\n",
    "    '''\n",
    "    Synopsis:\n",
    "        loss= loss_weighted_error( out, y )\n",
    "    Arguments:\n",
    "        out:  output of the classifier\n",
    "        y:    true class labels\n",
    "    Output:\n",
    "        loss: weighted error\n",
    "    '''\n",
    "    \n",
    "    labels = [0,1]\n",
    "    err0, err1 = [(out[y==lab] - y[y==lab]).mean()**2 if len(y[y==lab]) != 0 else 0 \\\n",
    "                  for lab in labels]\n",
    "    \n",
    "    return .5 * (err0 + err1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'erp_hexVPsag.npz'\n",
    "cnt, fs, clab, mnt, mrk_pos, mrk_class, mrk_className = bci.load_data(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Classification of Temporal Features  (2 points)\n",
    "Extract as temporal features from single channels the epochs of the time interval 0 to 1000 ms. Determine the error of classification with LDA and with NCC on those features using 10-fold cross-validation for each single channel. Display the resulting (test) error rates for all channel as scalp topographies (one for LDA and one for NCC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 101 but corresponding boolean dimension is 1079",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-cf16499d1c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_NCC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmrk_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4bb01031f3e8>\u001b[0m in \u001b[0;36mcrossvalidation\u001b[0;34m(classifier_fcn, X, y, nFolds, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0midxTe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mff\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0midxTr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnSamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxTe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxTr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxTr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0merrTe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_weighted_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxTe\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxTe\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e91beccd5e10>\u001b[0m in \u001b[0;36mtrain_NCC\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmean1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#Calculate weights and bias:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-e91beccd5e10>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmean1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#Calculate weights and bias:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 101 but corresponding boolean dimension is 1079"
     ]
    }
   ],
   "source": [
    "ival = np.array([0,1000]) #interval in ms\n",
    "lower, upper = ival*fs/1000\n",
    "\n",
    "epo, epo_t = bci.makeepochs(cnt, fs, mrk_pos, ival)\n",
    "epo.shape\n",
    "\n",
    "for i in range(epo.shape[1]):\n",
    "    temp = epo[:, i, :]\n",
    "    test, training = crossvalidation(train_NCC, temp , mrk_class)\n",
    "    print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Classification of Spatial Features  (3 points)\n",
    "Perform classification (*target* vs. *nontarget*) on spatial features (average across time within a 50 ms interval) in a time window that is shifted from 0 to 1000 ms in steps of 10 ms, again with both, LDA and NCC. Visualize the time courses of the classification error. Again, use 10-fold cross-validation. Here, use a baseline correction w.r.t. the prestimulus interval -100 to 0 ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = np.ones(5)*2\n",
    "a = np.hstack((a,3))\n",
    "np.unique(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
