{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Intelligence 6: Deep Neural Networks\n",
    "Belugas on the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x635857a50>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#Just checking out the data:\n",
    "\n",
    "plt.figure(figsize = (5,5))\n",
    "plt.imshow(x_train[np.random.randint(0, len(x_train))], cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_63 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_169 (Dense)            (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/17\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3950 - accuracy: 0.8872\n",
      "Epoch 2/17\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.3063 - accuracy: 0.9133\n",
      "Epoch 3/17\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2918 - accuracy: 0.9168\n",
      "Epoch 4/17\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.2848 - accuracy: 0.9199\n",
      "Epoch 5/17\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2784 - accuracy: 0.9221\n",
      "Epoch 6/17\n",
      "60000/60000 [==============================] - 1s 12us/sample - loss: 0.2754 - accuracy: 0.9226\n",
      "Epoch 7/17\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2726 - accuracy: 0.9238\n",
      "Epoch 8/17\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2700 - accuracy: 0.9241\n",
      "Epoch 9/17\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.2673 - accuracy: 0.9256\n",
      "Epoch 10/17\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2661 - accuracy: 0.9258\n",
      "Epoch 11/17\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.2637 - accuracy: 0.9261\n",
      "Epoch 12/17\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2633 - accuracy: 0.9262\n",
      "Epoch 13/17\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.2618 - accuracy: 0.9271\n",
      "Epoch 14/17\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2609 - accuracy: 0.9273\n",
      "Epoch 15/17\n",
      "60000/60000 [==============================] - 1s 13us/sample - loss: 0.2602 - accuracy: 0.9267\n",
      "Epoch 16/17\n",
      "60000/60000 [==============================] - 1s 15us/sample - loss: 0.2580 - accuracy: 0.9277\n",
      "Epoch 17/17\n",
      "60000/60000 [==============================] - 1s 14us/sample - loss: 0.2578 - accuracy: 0.9285\n",
      "60000/1 - 1s - loss: 0.1748 - accuracy: 0.9304\n",
      "10000/1 - 0s - loss: 0.1982 - accuracy: 0.9224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27771653480827807, 0.9224]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part (1): Train a linear model\n",
    "\n",
    "lin_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(10, activation='softmax',\n",
    "                        kernel_initializer = 'zeros',\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = 'zeros')\n",
    "])\n",
    "\n",
    "lin_model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Getting summary of the model:\n",
    "\n",
    "lin_model.summary()\n",
    "\n",
    "#Set the learning rate:\n",
    "\n",
    "lin_model.optimizer.lr = .5\n",
    "\n",
    "#Fit the model:\n",
    "\n",
    "lin_model.fit(x_train, y_train, batch_size = 100, epochs = 17)\n",
    "\n",
    "#Evaluate the model:\n",
    "\n",
    "lin_model.evaluate(x_train, y_train, verbose = 2) #on training set\n",
    "lin_model.evaluate(x_test, y_test, verbose=2) #on hold-out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([-1.0287154 ,  0.8822517 ,  0.2294641 , -0.6019657 ,  0.10574502,\n",
       "         2.273805  , -0.37636167,  1.3453046 , -2.3290944 , -0.50042796],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wghts = lin_model.get_weights()\n",
    "wghts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran stochastic gradient descent on the training set for $\\frac{10000 \\cdot 100}{60000} \\approx 17$ epochs. The accuracies of the linear model on the test- and hold-out set are comparable. On the test set it has an accuracy of 93.12%, with a performance of 92.46% on the the hold-out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_90\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_56 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 1500)              1177500   \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 10)                15010     \n",
      "=================================================================\n",
      "Total params: 5,695,510\n",
      "Trainable params: 5,695,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Part (2): Train a Multi-layer Perceptron:\n",
    "\n",
    "#Defining initializers for layers:\n",
    "\n",
    "trunc_normal = tf.keras.initializers.TruncatedNormal(mean=0, stddev=.01)\n",
    "small_bias = tf.keras.initializers.constant(value = .1)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=.001, \n",
    "                             beta_1= .9, \n",
    "                             beta_2= .999, \n",
    "                             epsilon = 1e-07,\n",
    "                             amsgrad=False)\n",
    "\n",
    "mlp_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(1500, activation= 'relu',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias),\n",
    "    tf.keras.layers.Dense(1500, activation= 'relu',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias),\n",
    "    tf.keras.layers.Dense(1500, activation= 'relu',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias),\n",
    "    tf.keras.layers.Dense(10, activation= 'softmax',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias)\n",
    "\n",
    " ])\n",
    "\n",
    "new_model = tf.kerals \n",
    "\n",
    "#Summarizing the model:\n",
    "\n",
    "mlp_model.summary()\n",
    "\n",
    "#Weights after training:\n",
    "\n",
    "mlp_wghts = mlp_model.get_weights()\n",
    "#mlp_wghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model:\n",
    "\n",
    "mlp_model.compile(optimizer= adam,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Fitting the model:\n",
    "\n",
    "mlp_model.fit(x_train, y_train, batch_size = 100, epochs = 34)\n",
    "\n",
    "#Evaluate the model:\n",
    "\n",
    "mlp_model.evaluate(x_train, y_train, verbose = 2) #on training set\n",
    "mlp_model.evaluate(x_test, y_test, verbose=2) #on hold-out se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00805286  0.00753473  0.00910047 ...  0.01426506 -0.01038765\n",
      "  -0.00814988]\n",
      " [ 0.00776395 -0.00252953 -0.00112258 ... -0.00355989  0.00945012\n",
      "  -0.00298003]\n",
      " [-0.00469809 -0.00160829 -0.00199437 ... -0.01006735  0.0193077\n",
      "  -0.00521336]\n",
      " ...\n",
      " [-0.00416496 -0.01915614 -0.00061498 ...  0.00109299  0.00871983\n",
      "  -0.00250383]\n",
      " [-0.00195093  0.0041697  -0.00619997 ...  0.01167369  0.00381235\n",
      "   0.00070498]\n",
      " [-0.00150265 -0.00027623  0.01187981 ...  0.0183203  -0.00428199\n",
      "   0.00087309]]\n"
     ]
    }
   ],
   "source": [
    "#Get the weight matrix and the trained biases:\n",
    "\n",
    "wght_matr = mlp_model.layers[1].get_weights()[0]\n",
    "bias = mlp_model.layers[1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_57 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_160 (Dense)            (None, 1500)              1177500   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_162 (Dense)            (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_163 (Dense)            (None, 10)                15010     \n",
      "=================================================================\n",
      "Total params: 5,695,510\n",
      "Trainable params: 5,695,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Part (3): \n",
    "\n",
    "mlp_drpt_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(1500, activation= 'relu',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    tf.keras.layers.Dense(1500, activation= 'relu',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    tf.keras.layers.Dense(1500, activation= 'relu',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias),\n",
    "    tf.keras.layers.Dropout(.5),\n",
    "    tf.keras.layers.Dense(10, activation= 'softmax',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias)\n",
    "\n",
    " ])      \n",
    "\n",
    "#Summarizing the model:\n",
    "\n",
    "mlp_drpt_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same old, same old:\n",
    "\n",
    "#Compiling the model:\n",
    "\n",
    "mlp_drpt_model.compile(optimizer= adam,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Fitting the model:\n",
    "\n",
    "mlp_drpt_model.fit(x_train, y_train, batch_size = 100, epochs = 34)\n",
    "\n",
    "#Evaluate the model:\n",
    "\n",
    "mlp_drpt_model.evaluate(x_train, y_train, verbose = 2) #on training set\n",
    "mlp_drpt_model.evaluate(x_test, y_test, verbose=2) #on hold-out set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_52 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_50 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_51 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_58 (Flatten)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 10)                31370     \n",
      "=================================================================\n",
      "Total params: 83,466\n",
      "Trainable params: 83,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Part (4):\n",
    "\n",
    "#Reshaping data, to allow convolution:\n",
    "\n",
    "x_train_conv = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test_conv = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "shape_of_input = (28, 28, 1)\n",
    "\n",
    "#Getting started on the network:\n",
    "\n",
    "conv_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (5,5), strides = (1,1), padding = 'same',\n",
    "                           input_shape = shape_of_input,\n",
    "                           activation = 'relu',\n",
    "                           kernel_initializer = trunc_normal,\n",
    "                           use_bias = True,\n",
    "                           bias_initializer = small_bias),\n",
    "    tf.keras.layers.MaxPooling2D((2,2,), strides = (2,2)),\n",
    "    tf.keras.layers.Conv2D(64, (5,5), strides = (1,1), padding = 'same',\n",
    "                           input_shape = shape_of_input,\n",
    "                           activation = 'relu',\n",
    "                           kernel_initializer = trunc_normal,\n",
    "                           use_bias = True,\n",
    "                           bias_initializer = small_bias),\n",
    "    tf.keras.layers.MaxPooling2D((2,2,), strides = (2,2)),\n",
    "    tf.keras.layers.Flatten(), #flatten before feeding to CNN layer\n",
    "    tf.keras.layers.Dense(10, activation= 'softmax',\n",
    "                        kernel_initializer = trunc_normal,\n",
    "                        use_bias = True,\n",
    "                        bias_initializer = small_bias)\n",
    "    \n",
    "])\n",
    "\n",
    "#Get a summary of the model \n",
    "\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model:\n",
    "\n",
    "conv_model.compile(optimizer= adam,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Fitting the model:\n",
    "\n",
    "conv_model.fit(x_train_conv, y_train_conv, batch_size = 100, epochs = 34)\n",
    "\n",
    "#Evaluate the model:\n",
    "\n",
    "conv_model.evaluate(x_train_conv, y_train, verbose = 2) #on training set\n",
    "conv_model.evaluate(x_test_conv, y_test, verbose=2) #on hold-out se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part (5) â€“ Compare the results:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
