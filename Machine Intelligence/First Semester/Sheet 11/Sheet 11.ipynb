{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.svm import NuSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ridge = np.loadtxt('TrainingRidge.csv', delimiter = ',',\\\n",
    "                            skiprows = 1)\n",
    "validation_ridge = np.loadtxt('ValidationRidge.csv', delimiter = ',',\\\n",
    "                            skiprows = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Part[a]: Training the $\\nu$-SVR}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model (clf, validation_ridge, training_ridge):\n",
    "    \n",
    "    params = clf.get_params(deep = True)\n",
    "    \n",
    "    clf.fit(training_ridge[:,:2], training_ridge[:,2])\n",
    "    \n",
    "    prediction = clf.predict(np.array([validation_ridge[:,0], \\\n",
    "                                    validation_ridge[:,1]]).T)\n",
    "    \n",
    "    pairwise_err = np.power(validation_ridge[:,2] - prediction, 2)\n",
    "    \n",
    "    mse = sklearn.metrics.mean_squared_error(validation_ridge[:,2], prediction)\n",
    "    \n",
    "    score = clf.score(validation_ridge[:,:2], validation_ridge[:,2])\n",
    "    \n",
    "    return {\"model\": clf, \"pred\": prediction, \"err\": mse, \\\n",
    "            \"score\": score, \"par\": params, \"p_err\": pairwise_err}\n",
    "\n",
    "clf = NuSVR(gamma = \"auto\")\n",
    "\n",
    "print(\"The mean squared error for the model on the validation set is: {}\"\\\n",
    "              .format(run_model(clf, validation_ridge, \\\n",
    "                                training_ridge)[\"p_err\"]))\n",
    "\n",
    "def plot_colorMap (clf, validation_ridge = validation_ridge):\n",
    "    \n",
    "    result = run_model(clf, validation_ridge, training_ridge)\n",
    "    \n",
    "    prediction, mse, err = result[\"pred\"], result[\"err\"], result[\"p_err\"]\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2, sharex = True, sharey = True, figsize = (15,8))\n",
    "    \n",
    "    axs[1].scatter(training_ridge[:,0],\\\n",
    "                   training_ridge[:,1], \\\n",
    "                   c = training_ridge[:,2], \\\n",
    "                   marker = 'x',\n",
    "                   s = 1000,\n",
    "                   cmap = 'hot',\n",
    "                   label = 'Training Vectors')\n",
    "    axs[1].set_title('Model Prediction')\n",
    "    axs[1].set_xlabel('$X_1$')\n",
    "    axs[1].set_ylabel('$X_2$')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    \n",
    "    axs[0].scatter(validation_ridge[:,0],\\\n",
    "                   validation_ridge[:,1], \\\n",
    "                   c = validation_ridge[:,2], \\\n",
    "                   cmap = 'hot' )\n",
    "    axs[0].set_title('Validation Data')\n",
    "    axs[0].set_xlabel('$X_1$')\n",
    "    axs[0].set_ylabel('$X_2$')\n",
    "    \n",
    "    axs[1].scatter(validation_ridge[:,0],\\\n",
    "                   validation_ridge[:,1], \\\n",
    "                   c = prediction, \\\n",
    "                   cmap = 'hot',\n",
    "                   label = 'Training Vectors')\n",
    "    \n",
    "    plt.figure(figsize = (10,10))\n",
    "    \n",
    "    plt.scatter(validation_ridge[:,0],\\\n",
    "                validation_ridge[:,1],\\\n",
    "                c = err, \\\n",
    "                cmap = 'hot')\n",
    "    plt.title(\"Pointwise error of the model\")\n",
    "    plt.xlabel('$X_1$')\n",
    "    plt.ylabel('$X_2$')\n",
    "     \n",
    "plot_colorMap(NuSVR(gamma = 'auto'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Observation:}$ So, with an out of the box implementation, we do not get nearly enough  the performance we would wish. In particular, while we get decent performance on the training set, we do not get good performance on the data previously unseen by the model. The model is thus intensely overfitting. The lower plot shows the error per point in the plane. We can see that a naive regression model is performs worst in the high value areas. The main reason is that $C$ is not large enough and does not allow the model to be somewhat slack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Part[b]: Cross Validation}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_cross_validation (num_Fold, params, X = training_ridge):\n",
    "    \n",
    "    #========================================================================\n",
    "    \"\"\"\n",
    "    1. Fit the model to the X_train data\n",
    "    2. Get the score of the model on the Y_train test set\n",
    "    3. Average the score over all different splits.\n",
    "    4. Store the parameters and their average score\n",
    "    \n",
    "    \"\"\"\n",
    "    #======================================================================== \n",
    "    \n",
    "    kfold = KFold(n_splits = num_Fold, shuffle = True, random_state = 0)\n",
    "    \n",
    "    Cs, gams = np.meshgrid(params[\"C\"], params[\"Nu\"])\n",
    "    \n",
    "    params_matr = np.array([(Cs[i,j], gams[i,j]) for i in range(Cs.shape[0])\\\n",
    "                                                for j in range(Cs.shape[1])])\n",
    "    counter = 0\n",
    "    \n",
    "    scores = np.zeros(params_matr.shape[0]) \n",
    "    \n",
    "    for C, gam in params_matr:\n",
    "\n",
    "        mse = 0\n",
    "        \n",
    "        clf = NuSVR(gamma = gam, C = C)\n",
    "    \n",
    "        for train, test in kfold.split(X):\n",
    "            \n",
    "            X_train, X_test = X[train], X[test]\n",
    "            \n",
    "            result = run_model(clf, X_test, X_train)\n",
    "        \n",
    "            mse += result[\"err\"]\n",
    "            \n",
    "        scores[counter] = mse / num_Fold\n",
    "            \n",
    "        counter += 1 \n",
    "        \n",
    "    return scores, params_matr[np.argmin(scores)]\n",
    "\n",
    "#========================================================================\n",
    "\n",
    "#Define some parameter dictionaries for testing\n",
    "\n",
    "params_dict = {\"C\": [2**float(i) for i in np.arange(-2,12)], \\\n",
    "               \"Nu\": [2**float(j) for j in np.arange(-12,0)]}\n",
    "\n",
    "result = kfold_cross_validation(10, params_dict)\n",
    "\n",
    "print(r\"The best parameter configuration is: C = {}, $\\gamma$ = {}\"\\\n",
    "                      .format(result[1][0], result[1][1]))\n",
    "\n",
    "#========================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Part[c]: Re-training the entire network on optimal parameters}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(params):\n",
    "    \n",
    "    result = kfold_cross_validation(10, params)\n",
    "\n",
    "    print(r\"The best parameter configuration is: C = {}, $\\nu$ = {}\"\\\n",
    "                      .format(result[1][0], result[1][1]))\n",
    "    \n",
    "    clf = NuSVR(gamma = 'auto', C = result[1][0], nu = result[1][1])\n",
    "    \n",
    "    return run_model(clf, centred_val_data, centred_data)\n",
    "\n",
    "\n",
    "best_model_results = train_best_model(params_dict)\n",
    "print(best_model_results['err'])\n",
    "plot_colorMap(best_model_results[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Observation:}$ A couple of things are obvious. Firstly, the model performance increases; the MSE goes down from the original implementation. But the improvement is achieved at the cost of getting predictions exactly right, especially on high-valued portion of the data. Rather, the improvement is achieved – as can be seen by comparing the top-right plots from now and earlier – by lowering the average prediction across $all$ the data (as can be surmised from the fact that the plot becomes much $\\underline{darker}$). The bottom plot shows that, nevertheless, the residual error remains pretty high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
